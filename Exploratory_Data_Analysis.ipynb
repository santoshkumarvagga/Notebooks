{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Very Imp Step.\n",
    "\n",
    "EDA : Exploratory Data Analysis, How does one explore data with the aim of analysing it.\n",
    "\n",
    "Objectives of EDA:\n",
    "\n",
    "* Trying to understand data much better so that we can get an handle around it.\n",
    "* TO try and see if the data has some intresting patterns(which may or may not be helpful for client), But EDA is about uncovering those.\n",
    "\n",
    "Steps of EDA:\n",
    "\n",
    "1) Data Sourcing : Process of Finding/Loading the data into your system.\n",
    "2) Data Cleansing : Clearing common issues like Encoding issues, repeated names but with different spells, Marks = 0, etc. Make such mistakes to follow a consistent naming. \n",
    "3) Univariate Analysis : If we consider only one variable (column) and analyse how it affected the random variable(variable of interest)\n",
    "4) Bivariate Analysis: If we consider two varibale (columns) and anlayse their dependency on random variable.\n",
    "5) Derived Metrics : Creating new columns from available data (columns) that could help in analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Data Sourcing: Getting(Sources) Data and Loading it.\n",
    "\n",
    "For any Problem on Analytics, Basic thing required is Historical Data.\n",
    "\n",
    "2 types of Data:\n",
    "a) Public b) Private\n",
    "\n",
    "a) Public: Easy to get, but often dont contain info we are intrested in, most of times.\n",
    "b) Private: Hard to get (from offical agencies, govt), contain useful data, most of times.\n",
    "\n",
    "* Public data sourcing: We can get from Internet Browsing for free of cost. Eg: awesome-public-datasets\n",
    "\n",
    "    NOTE: Understanding of Domain is must to see what we can fecth from Data.\n",
    "\n",
    "* private Data Sourcing : we get it from respective organisations directly following all the protocol.May include Money, Officials permission. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Data Cleaning: Once we have data, Clean it for data quality issues.\n",
    "\n",
    "* There are various types of data quality issues - formatting errors (e.g. rows and columns are ill-formatted, unclearly named etc.), missing values, repeated rows, spelling inconsistencies, etc. \n",
    "* These issues could make it difficult to analyse data and could lead to errors or irrelevant results. Thus, these issues need to be corrected before data is analysed.\n",
    "\n",
    "Data Cleaning steps:\n",
    "1) Fix rows and columns\n",
    "2) Fix missing values\n",
    "3) Standardise values\n",
    "4) Fix invalid values\n",
    "5) Filter data\n",
    "\n",
    "\n",
    "1)  Fix rows and columns: We keep only row/column values, not the headers, footers, sl.no, subtotal, etc\n",
    "    Refer this Checklist - https://drive.google.com/drive/folders/106ouzKgWWRrOfJwgSAQPINnU3Sc4BVyh\n",
    "\n",
    "2) Missing Values: Steps to deal with missing values are given below,\n",
    "* Set values as missing values: Identify values that indicate missing data, and yet are not recognised by the software as such, e.g treat blank strings, \"NA\", \"XX\", \"999\", etc. as missing.\n",
    "\n",
    "* Adding is good, exaggerating is bad: You should try to get information from reliable external sources as much as possible, but if you can’t, then it is better to keep missing values as such rather than exaggerating the existing rows/columns.\n",
    "\n",
    "* Delete rows, columns: Rows could be deleted if the number of missing values are significant in number, as this would not impact the analysis. Columns could be removed if the missing values are quite significant in number.\n",
    "\n",
    "* Fill partial missing values using business judgement: Missing time zone, century, etc. These values are easily identifiable.\n",
    "\n",
    "3) Standaridise Values: We have come to cell level. Convert all cells of a column to same unit(KMPH or Metres)\n",
    "    2 Types: \n",
    "\n",
    "        a) Standaridising Variables(numeic): steps to deal with are,\n",
    "\n",
    "            * Standardise units: Ensure all observations under a variable have a common and consistent unit, e.g. convert lbs to kgs,               miles/hr to km/hr, etc.\n",
    "\n",
    "            * Scale values if required:  Make sure the observations under a variable have a common scale\n",
    "\n",
    "            * Standardise precision for better presentation of data, e.g. 4.5312341 kgs to 4.53 kgs.\n",
    "\n",
    "            * Remove outliers: Remove high and low values that would disproportionately affect the results of your analysis.\n",
    "\n",
    "\n",
    "        b) Standaridising Text: steps to dela with are:\n",
    "\n",
    "            * Remove extra characters like such as common prefix/suffix, leading/trailing/multiple spaces, etc. These are irrelevant                to analysis.\n",
    "\n",
    "            * Standardise case: There are various cases that string variables may take, e.g. UPPERCASE, lowercase, Title Case,                      Sentence case, etc.\n",
    "\n",
    "            * Standardise format: E.g. 23/10/16 to 2016/10/23, “Modi, Narendra\" to “Narendra Modi\", etc.\n",
    "\n",
    "        NOTE: Why Outlier treatment is important? https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html\n",
    "\n",
    "4) Invalid Values: steps to deal with,\n",
    "\n",
    "* Encode unicode properly: In case the data is being read as junk characters, try to change encoding, E.g. CP1252 instead of UTF-8.\n",
    "* Convert incorrect data types: Correct the incorrect data types to the correct data types for ease of analysis. E.g. if numeric values are stored as strings, it would not be possible to calculate metrics such as mean, median, etc. Some of the common data type corrections are — string to number: \"12,300\" to “12300”; string to date: \"2013-Aug\" to “2013/08”; number to string: “PIN Code 110001” to \"110001\"; etc.\n",
    "* Correct values that go beyond range: If some of the values are beyond logical range, e.g. temperature less than -273° C (0° K), you would need to correct them as required. A close look would help you check if there is scope for correction, or if the value needs to be removed.\n",
    "* Correct values not in the list: Remove values that don’t belong to a list. E.g. In a data set containing blood groups of individuals, strings “E” or “F” are invalid values and can be removed.\n",
    "* Correct wrong structure: Values that don’t follow a defined structure can be removed. E.g. In a data set containing pin codes of Indian cities, a pin code of 12 digits would be an invalid value and needs to be removed. Similarly, a phone number of 12 digits would be an invalid value.\n",
    "* Validate internal rules: If there are internal rules such as a date of a product’s delivery must definitely be after the date of the order, they should be correct and consistent.\n",
    "\n",
    "5) Filtering Data(for the ease of Analysis):\n",
    "\n",
    "* Deduplicate data: Remove identical rows, remove rows where some columns are identical\n",
    "* Filter rows: Filter by segment, filter by date period to get only the rows relevant to the analysis\n",
    "* Filter columns: Pick columns relevant to the analysis\n",
    "* Aggregate data: Group by required keys, aggregate the rest\n",
    "\n",
    "Once we get specific parts of data to analyse, we are ready get insights into by analysing. Lets look how to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Univariate Analysis: analysing variables one at a time. It is important to separately understand each variable before moving on to analysing multiple variables together.\n",
    "\n",
    "Mainly:\n",
    "    * Metadata description\n",
    "    * Data distribution plots\n",
    "    * Summary metrics"
   ]
  }
 ]
}